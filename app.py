import streamlit as st
from dotenv import load_dotenv
from PyPDF2 import PdfReader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from htmlTemplates import css, bot_template, user_template
from langchain_community.chat_models import ChatZhipuAI
from langchain_community.embeddings import ZhipuAIEmbeddings
import os
from dotenv import load_dotenv, find_dotenv
import streamlit as st
import time


# 1. å°è¯•è‡ªåŠ¨åŠ è½½
loaded = load_dotenv(find_dotenv()) 
if not os.getenv("ZHIPUAI_API_KEY"):
    # å¤‡é€‰æ–¹æ¡ˆï¼šç›´æ¥åœ¨ä»£ç é‡Œç¡¬ç¼–ç  Key 
    # è¯·æŠŠä¸‹é¢å¼•å·é‡Œçš„å†…å®¹æ¢æˆä½ çœŸå®çš„ Key
    os.environ["ZHIPUAI_API_KEY"] = "" 

# --- è°ƒè¯•æ‰“å°ï¼ˆè¿è¡Œåçœ‹ç»ˆç«¯è¾“å‡ºï¼‰---
print(f"Env Loaded: {loaded}")
print(f"API Key Status: {'Found' if os.getenv('ZHIPUAI_API_KEY') else 'Missing'}")

def get_pdf_text(pdf_docs):
    text = ""
    for pdf in pdf_docs:
        try:
            pdf_reader = PdfReader(pdf)
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text += page_text
        except Exception as e:
            print(f"Error reading PDF file: {e}")
            continue
    return text

def get_text_chunks(text):
    # ä½¿ç”¨é€’å½’åˆ‡åˆ†å™¨ï¼Œå®ƒä¼šå°è¯• "\n\n", "\n", " " ç­‰å¤šç§åˆ†éš”ç¬¦ï¼Œä¿è¯åˆ‡åˆ†å‡åŒ€
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=500,       
        chunk_overlap=100,    
        length_function=len
    )
    chunks = text_splitter.split_text(text)
    return chunks


import time

import time

def get_vectorstore(text_chunks):
    # 1. åˆå§‹åŒ–æ™ºè°± Embedding
    try:
        embeddings = ZhipuAIEmbeddings(
            model="embedding-2",
            api_key=os.getenv("ZHIPUAI_API_KEY")
        )
    except Exception as e:
        st.error(f"API Key é…ç½®é”™è¯¯: {e}")
        return None
    
    
    # å»é™¤æ¢è¡Œç¬¦ã€å»é™¤é¦–å°¾ç©ºæ ¼ã€å¼ºåˆ¶è½¬ä¸ºå­—ç¬¦ä¸²
    clean_chunks = []
    for t in text_chunks:
        if t and isinstance(t, str):
            cleaned_t = t.replace("\n", " ").strip()
            if len(cleaned_t) > 0:
                clean_chunks.append(cleaned_t)
    
    if not clean_chunks:
        st.error("âŒ è­¦å‘Šï¼šæ–‡æ¡£æå–å†…å®¹ä¸ºç©ºï¼å¯èƒ½æ˜¯æ‰«æç‰ˆ PDFï¼ˆå›¾ç‰‡æ ¼å¼ï¼‰ã€‚è¯·ä¸Šä¼ æ–‡å­—ç‰ˆ PDFã€‚")
        return None

   
    batch_size = 10  
    vectorstore = None
    
    # åˆå§‹åŒ–è¿›åº¦æ¡
    progress_text = f"æ­£åœ¨å¤„ç† {len(clean_chunks)} æ¡æ–‡æœ¬ç‰‡æ®µ..."
    my_bar = st.progress(0, text=progress_text)
    
    for i in range(0, len(clean_chunks), batch_size):
        batch = clean_chunks[i : i + batch_size]
        
        if not batch:
            continue

        try:
            if vectorstore is None:
                vectorstore = FAISS.from_texts(texts=batch, embedding=embeddings)
            else:
                vectorstore.add_texts(batch)
            
            # æ‰“å°æˆåŠŸæ—¥å¿—ï¼ˆè°ƒè¯•ç”¨ï¼‰
            print(f"âœ… Batch {i} to {i+len(batch)} success.")
            
        except Exception as e:
            # æ‰“å°å¤±è´¥çš„å…·ä½“å†…å®¹ï¼Œæ–¹ä¾¿ä½ æˆªå›¾ç»™æˆ‘
            print(f"âŒ Batch {i} failed: {e}")
            print(f"   Sample content: {batch[0][:50]}...") # æ‰“å°è¿™ä¸€æ‰¹çš„ç¬¬ä¸€å¥è¯çœ‹çœ‹æ˜¯å•¥
            # ä¸è¦ continueï¼Œå°è¯•è®©å®ƒå¤±è´¥ï¼Œå¦åˆ™ vectorstore ä¸ºç©ºåé¢è¿˜æ˜¯ä¼šå´©
            # ä½†ä¸ºäº†ä¸å¡æ­»ï¼Œæˆ‘ä»¬è¿™é‡Œé€‰æ‹©è·³è¿‡
            continue
            
        # æ›´æ–°è¿›åº¦
        current_progress = min((i + batch_size) / len(clean_chunks), 1.0)
        my_bar.progress(current_progress, text=f"{progress_text} ({int(current_progress*100)}%)")
        
        # å¢åŠ ä¼‘æ¯æ—¶é—´ï¼Œé˜²æ­¢ QPS è¶…é™
        time.sleep(0.3) 
        
    my_bar.empty()
    return vectorstore

def get_conversation_chain(vectorstore):
    # ä½¿ç”¨æ™ºè°± GLM-4 æ¨¡å‹
    llm = ChatZhipuAI(
        model="glm-4",
        temperature=0.1,
        api_key=os.getenv("ZHIPUAI_API_KEY")
    )

    memory = ConversationBufferMemory(
        memory_key='chat_history', return_messages=True)
    
    conversation_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        memory=memory
    )
    return conversation_chain

def handle_userinput(user_question):
    response = st.session_state.conversation({'question': user_question})
    st.session_state.chat_history = response['chat_history']

    for i, message in enumerate(st.session_state.chat_history):
        if i % 2 == 0:
            st.write(user_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)
        else:
            st.write(bot_template.replace(
                "{{MSG}}", message.content), unsafe_allow_html=True)


def main():
    st.set_page_config(page_title="Financial Analyst Agent", page_icon="ğŸ“ˆ")
    
   
    st.write(css, unsafe_allow_html=True)
    st.header("ğŸ“ˆ Intelligent Financial Report Analyst")
    st.markdown("##### åŸºäº LangChain + RAG çš„é‡‘èç ”æŠ¥æ™ºèƒ½åˆ†æç³»ç»Ÿ")

    # --- åˆå§‹åŒ– session state ---
    if "conversation" not in st.session_state:
        st.session_state.conversation = None
    if "chat_history" not in st.session_state:
        st.session_state.chat_history = None

    # --- ä¸»ç•Œé¢ï¼šæ˜¾ç¤ºèŠå¤©è®°å½• ---
    # å¦‚æœæœ‰èŠå¤©è®°å½•ï¼Œå€’åºæ˜¾ç¤ºï¼ˆç¬¦åˆç›´è§‰ï¼‰
    if st.session_state.chat_history:
        for i, message in enumerate(st.session_state.chat_history):
            if i % 2 == 0:
                st.write(user_template.replace(
                    "{{MSG}}", message.content), unsafe_allow_html=True)
            else:
                st.write(bot_template.replace(
                    "{{MSG}}", message.content), unsafe_allow_html=True)

    # --- åº•éƒ¨è¾“å…¥æ¡† ---
    user_question = st.text_input("å‘ AI åˆ†æå¸ˆæé—® (ä¾‹å¦‚ï¼šè¿™å®¶å…¬å¸çš„ä¸»è¦é£é™©æ˜¯ä»€ä¹ˆ?):")
    if user_question:
        handle_userinput(user_question)

    # --- ä¾§è¾¹æ  (æ ¸å¿ƒåŠŸèƒ½åŒº) ---
    with st.sidebar:
        st.subheader("ğŸ“š ç ”æŠ¥çŸ¥è¯†åº“")
        pdf_docs = st.file_uploader(
            "ä¸Šä¼ ç ”æŠ¥ (PDFæ ¼å¼)", accept_multiple_files=True)
        
        if st.button("Process (åˆå§‹åŒ–çŸ¥è¯†åº“)"):
            with st.spinner("æ­£åœ¨è¿›è¡Œå‘é‡åŒ–åˆ‡ç‰‡ä¸æ„å»ºç´¢å¼•..."):
                # 1. è·å– PDF æ–‡æœ¬
                raw_text = get_pdf_text(pdf_docs)
                
                # 2. æ–‡æœ¬åˆ†å—
                text_chunks = get_text_chunks(raw_text)
                
                if not text_chunks:
                    st.error("æ— æ³•ä» PDF ä¸­æå–æ–‡å­—ã€‚")
                else:
                    # 3. å‘é‡åŒ–å­˜å‚¨
                    vectorstore = get_vectorstore(text_chunks)
                    
                    # --- [å…³é”®ä¿®å¤] å®‰å…¨æ£€æŸ¥ ---
                    if vectorstore is None:
                        st.error("âŒ çŸ¥è¯†åº“æ„å»ºå¤±è´¥ã€‚è¯·æŸ¥çœ‹ç»ˆç«¯(Terminal)é‡Œçš„å…·ä½“æŠ¥é”™ä¿¡æ¯ã€‚")
                    else:
                        # 4. åªæœ‰ vectorstore å­˜åœ¨æ—¶ï¼Œæ‰åˆ›å»ºå¯¹è¯é“¾
                        st.session_state.conversation = get_conversation_chain(vectorstore)
                        st.success("âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼")

       
        st.markdown("---")
        st.subheader("ğŸ“Š æ™ºèƒ½åˆ†æå·¥å…·ç®±")
        analysis_task = st.selectbox(
            "é€‰æ‹©è‡ªåŠ¨åŒ–åˆ†æä»»åŠ¡",
            ["è¯·é€‰æ‹©...", "ğŸ“‹ ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹æ‘˜è¦", "ğŸ’° æå–ç›ˆåˆ©é¢„æµ‹æ•°æ®", "âš ï¸ è¯†åˆ«æ½œåœ¨é£é™©å› ç´ ", "ğŸŒ¡ï¸ å¸‚åœºæƒ…ç»ªé‡åŒ–æ‰“åˆ†"]
        )

        if analysis_task != "è¯·é€‰æ‹©...":
            if st.session_state.conversation is None:
                st.error("è¯·å…ˆä¸Šä¼  PDF å¹¶ç‚¹å‡» Processï¼")
            else:
                if st.button(f"æ‰§è¡Œï¼š{analysis_task}"):
                    with st.spinner("AI åˆ†æå¸ˆæ­£åœ¨é˜…è¯»ç ”æŠ¥å¹¶ç”ŸæˆæŠ¥å‘Š..."):
                        # å®šä¹‰ä¸“ä¸šçš„ Prompt (å¯¹åº”ç®€å†ä¸­çš„ Prompt Engineering)
                        prompts = {
                            "ğŸ“‹ ç”Ÿæˆæ ¸å¿ƒè§‚ç‚¹æ‘˜è¦": "ä½ æ˜¯ä¸€åèµ„æ·±è¯åˆ¸åˆ†æå¸ˆã€‚è¯·é˜…è¯»è¿™ç¯‡ç ”æŠ¥ï¼Œç”¨ä¸“ä¸šçš„é‡‘èæœ¯è¯­æ€»ç»“æ–‡ç« çš„æ ¸å¿ƒæŠ•èµ„é€»è¾‘ã€æ¨èç†ç”±ä»¥åŠç›®æ ‡ä»·ã€‚å­—æ•°æ§åˆ¶åœ¨300å­—ä»¥å†…ã€‚",
                            "ğŸ’° æå–ç›ˆåˆ©é¢„æµ‹æ•°æ®": "è¯·ä»æ–‡ä¸­æå–æœªæ¥3å¹´çš„å…³é”®è´¢åŠ¡é¢„æµ‹æ•°æ®ï¼ˆå¦‚è¥æ”¶ Revenueã€å‡€åˆ©æ¶¦ Net Profitã€EPSç­‰ï¼‰ï¼Œå¹¶ä»¥ Markdown è¡¨æ ¼å½¢å¼åˆ—å‡ºã€‚å¦‚æœæ–‡ä¸­æ²¡æœ‰å…·ä½“æ•°å­—ï¼Œè¯·è¯´æ˜ã€‚",
                            "âš ï¸ è¯†åˆ«æ½œåœ¨é£é™©å› ç´ ": "è¯·åˆ—å‡ºè¿™ç¯‡ç ”æŠ¥ä¸­æåˆ°çš„å‰3å¤§æŠ•èµ„é£é™©ï¼ˆRisksï¼‰ï¼Œå¹¶è¯„ä¼°å…¶å¯¹è‚¡ä»·çš„æ½œåœ¨å½±å“ç¨‹åº¦ï¼ˆé«˜/ä¸­/ä½ï¼‰ã€‚",
                            "ğŸŒ¡ï¸ å¸‚åœºæƒ…ç»ªé‡åŒ–æ‰“åˆ†": "åŸºäºè¿™ç¯‡ç ”æŠ¥çš„æªè¾å¼ºç¡¬ç¨‹åº¦å’Œæ¨èè¯„çº§ï¼Œç»™è¯¥è‚¡ç¥¨çš„å¸‚åœºæƒ…ç»ªæ‰“åˆ†ï¼ˆ0-10åˆ†ï¼Œ10åˆ†ä¸ºæåº¦çœ‹æ¶¨ï¼‰ï¼Œå¹¶ç®€è¿°æ‰“åˆ†ç†ç”±ã€‚"
                        }
                        
                        # è°ƒç”¨ç°æœ‰å¯¹è¯é“¾è¿›è¡Œåˆ†æ
                        prompt_content = prompts[analysis_task]
                        response = st.session_state.conversation({'question': prompt_content})
                        
                        # åœ¨ä¾§è¾¹æ ç›´æ¥å±•ç¤ºç»“æœï¼Œæˆ–è€…åœ¨ä¸»ç•Œé¢å±•ç¤º
                        st.markdown(f"### {analysis_task} ç»“æœ")
                        st.info(response['answer'])


if __name__ == '__main__':
    main()
